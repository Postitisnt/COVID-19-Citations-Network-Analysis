{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the required packages and libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from numba import jit\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the metadata JSON file in order to build a dictionary and assign to each article a unique identifier (different from the DOI for easiest management of the network).\n",
    "\n",
    "- metadata_dict -> contains all the articles and their data\n",
    "- nodes -> dictionary containing tuples to map from DOI to node_id and journal title\n",
    "- journals_dict -> dictionary to map from Journal_title to unique_id of the journal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read metadata JSON file in order to build a dictionary\n",
    "metadata = open(\"../Data/metadata.json\")\n",
    "metadata_dict = json.load(metadata)\n",
    "\n",
    "# Create a dict of pairs \"doi: (node_id, journal_title)\"\n",
    "nodes = dict()\n",
    "reverse_nodes = dict()\n",
    "\n",
    "# Create a dict of pairs \"Journal: unique_identifier\"\n",
    "journals_dict = {}\n",
    "\n",
    "# Add a number as unique identifier of each one of the papers and to each Journal\n",
    "i = 0\n",
    "j = 0\n",
    "for paper in metadata_dict:\n",
    "    new_journal = False\n",
    "    paper[\"node_id\"] = i\n",
    "    nodes[paper['id']] = (paper['node_id'], paper['source_title'])\n",
    "    if paper['source_title'] not in journals_dict:\n",
    "        journals_dict[paper['source_title']] = j\n",
    "        reverse_nodes[paper['node_id']] = (paper['id'], paper['source_title'], j)\n",
    "        new_journal = True\n",
    "    else:\n",
    "        idx = journals_dict[paper['source_title']]\n",
    "        reverse_nodes[paper['node_id']] = (paper['id'], paper['source_title'], idx)\n",
    "    i+=1\n",
    "    # art_id : (doi, journ_title, journ_id)\n",
    "    if new_journal:\n",
    "        j+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the first network made up of articles.</br>\n",
    "Also build the undirected network to analyze the structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "undirected_papers_network = nx.Graph()\n",
    "papers_network = nx.DiGraph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read citations JSON file in order to build a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "citations = open('../Data/citations.json')\n",
    "citations_dict = json.load(citations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate over citations_dict to build a papers citations' network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 189697/189697 [00:00<00:00, 250989.38it/s]\n"
     ]
    }
   ],
   "source": [
    "for citation_obj in tqdm(citations_dict):\n",
    "    source = citation_obj['source']\n",
    "    target = citation_obj['target']\n",
    "    if source in nodes:\n",
    "        if target in nodes:\n",
    "            source_article_id = nodes[source][0]\n",
    "            target_article_id = nodes[target][0]\n",
    "            undirected_papers_network.add_edge(source_article_id, target_article_id)\n",
    "            papers_network.add_edge(source_article_id, target_article_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the undirected papers' network for the structural analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_gml(undirected_papers_network, \"../gml format networks/undirected_papers_network.gml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the <i>PageRank</i> value of the nodes of our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('10.1056/nejmoa030781', 'New England Journal Of Medicine', 112)\n",
      "('10.1056/nejmoa030747', 'New England Journal Of Medicine', 112)\n",
      "('10.1016/s0140-6736(03)13077-2', 'The Lancet', 82)\n",
      "('10.1056/nejmoa1211721', 'New England Journal Of Medicine', 112)\n",
      "('10.1126/science.1085952', 'Science', 44)\n",
      "('10.1016/s0140-6736(03)13412-5', 'The Lancet', 82)\n",
      "('10.1038/nm1080', 'Nature Medicine', 265)\n",
      "('10.1126/science.1087139', 'Science', 44)\n",
      "('10.1038/sj.cr.7290286', 'Cell Research', 457)\n",
      "('10.1126/science.1085953', 'Science', 44)\n"
     ]
    }
   ],
   "source": [
    "page_rank = nx.pagerank(papers_network, alpha=0.85)\n",
    "pr_list = sorted(page_rank.items(), key=lambda item: item[1], reverse=True)\n",
    "for article in pr_list[:10]:\n",
    "    #print(article)\n",
    "    print(reverse_nodes[article[0]])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test to see which are the most important articles at this point, retrieved with the <i>Eigenvector Centrality</i> measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('10.1093/infdis/120.5.576', 'Journal Of Infectious Diseases', 414)\n",
      "('10.1016/s0140-6736(75)93176-1', 'The Lancet', 82)\n",
      "('10.1177/030098587301000105', 'Veterinary Pathology', 493)\n",
      "('10.1016/0014-4800(76)90045-9', 'Experimental And Molecular Pathology', 2002)\n",
      "('10.1093/oxfordjournals.aje.a121171', 'American Journal Of Epidemiology', 849)\n",
      "('10.1007/bf01253886', 'Archiv F�R Die Gesamte Virusforschung', 613)\n",
      "('10.1016/0042-6822(72)90062-1', 'Virology', 52)\n",
      "('10.1177/0300985871008005-00612', 'Veterinary Pathology', 493)\n",
      "('10.1016/0042-6822(77)90489-5', 'Virology', 52)\n",
      "('10.1136/bmj.1.5448.1467', 'Bmj', 612)\n"
     ]
    }
   ],
   "source": [
    "eigenvector_centrality = nx.eigenvector_centrality(papers_network, max_iter=1000)\n",
    "ec_list = sorted(eigenvector_centrality.items(), key=lambda item: item[1], reverse=True)\n",
    "for article in ec_list[:10]:\n",
    "    #print(article)\n",
    "    print(reverse_nodes[article[0]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test to see which are the most important article at this point with In-Degree count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('10.1056/nejmoa030781', 'New England Journal Of Medicine', 112)\n",
      "('10.1056/nejmoa1211721', 'New England Journal Of Medicine', 112)\n",
      "('10.1056/nejmoa030747', 'New England Journal Of Medicine', 112)\n",
      "('10.1016/s0140-6736(03)13077-2', 'The Lancet', 82)\n",
      "('10.1126/science.1085952', 'Science', 44)\n",
      "('10.1126/science.1085953', 'Science', 44)\n",
      "('10.1038/nature02145', 'Nature', 129)\n",
      "('10.1126/science.1087139', 'Science', 44)\n",
      "('10.1016/s0140-6736(03)13412-5', 'The Lancet', 82)\n",
      "('10.1038/nm1024', 'Nature Medicine', 265)\n"
     ]
    }
   ],
   "source": [
    "in_degree_dict = {}\n",
    "in_degree_iterable = papers_network.in_degree()\n",
    "for tup in in_degree_iterable:\n",
    "    node = tup[0]\n",
    "    in_degree = tup[1]\n",
    "    in_degree_dict[node] = in_degree\n",
    "in_d_list = sorted(in_degree_dict.items(), key=lambda item: item[1], reverse=True)\n",
    "for article in in_d_list[:10]:\n",
    "    #print(article)\n",
    "    print(reverse_nodes[article[0]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define a scaling function in order to scale values coming from different functions into values in the range [-1, 1]. The reason to do that is to get a visual grasp on the differences between the outcomes of the different algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def scaling(data, range_min, range_max):\n",
    "    data_values = []\n",
    "    for tup in data:\n",
    "        data_values.append(tup[1])\n",
    "    data_min = min(data_values)\n",
    "    data_max = max(data_values)\n",
    "    data_range = data_max - data_min\n",
    "    new_range = range_max - range_min\n",
    "    new_data = []\n",
    "    for tup in data:\n",
    "        scaled_value = (((tup[1] - data_min) * new_range) / data_range) + range_min\n",
    "        new_data.append((tup[0], scaled_value))\n",
    "    return new_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to sort lists of tuples by means of the first element, in our case it will be the node_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_list_of_tuples(data):\n",
    "    return sorted(data, key=lambda x: x[0], reverse=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale values from PageRank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, -0.9997101926143466), (1, -0.9999725867395567), (2, -0.9996765771504587)]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_pagerank = scaling(pr_list, -1, 1)\n",
    "scaled_pagerank = sort_list_of_tuples(scaled_pagerank)\n",
    "pagerank_array = [el[1] for el in scaled_pagerank]\n",
    "scaled_pagerank[:3]\n",
    "#pagerank_array"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale values from Eigenvector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, -1.0), (1, -1.0), (2, -1.0)]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_eigenvector = scaling(ec_list, -1, 1)\n",
    "scaled_eigenvector = sort_list_of_tuples(scaled_eigenvector)\n",
    "eigenvector_array = [el[1] for el in scaled_eigenvector]\n",
    "scaled_eigenvector[:3]\n",
    "#eigenvector_array"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale values from In-degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, -0.9980506822612085), (1, -0.9980506822612085), (2, -0.9980506822612085)]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_in_degree = scaling(in_d_list, -1, 1)\n",
    "scaled_in_degree = sort_list_of_tuples(scaled_in_degree)\n",
    "in_degree_array = [el[1] for el in scaled_in_degree]\n",
    "scaled_in_degree[:3] \n",
    "#in_degree_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order:\n",
    "- Read the JSON file containing citations' pairs;\n",
    "- Create a dictionary called \"journal_citations\" to store the different citations from journal to journal. The structure of this dictonary will be: \"citing_journal_id: list_of_cited_journal_ids\" (obviously, in the list we have repetitions of cited journals if articles cites more than one paper of the target journal);\n",
    "- Populate the network as said above. This is accomplished thanks to a temporary \"memo\" dict that stores each citations to every target journal and that is initialized every time the source journal changes.\n",
    "- Populate the \"weights\" dictionary. Such dictionary will contain the weight of each specific path retrieved and will be used to assign edge attributes to the network.\n",
    "- article_citations contains pairs of \"source article:[list of cited articles]\"."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">To retrieve the importance of edges in between journals:\n",
    "- $\\tau_j$ = PageRank centrality score\n",
    "- $\\Phi_J$ = importance of a journal\n",
    "- $j$ = article\n",
    "- $n_j$ = # articles in journal J\n",
    "- $n_{c_{AB}}$ = # of citations from journal A to journal B\n",
    "$$\\Phi_J = \\sum_{i=0}^{n_j} \\tau_i$$\n",
    "</br>\n",
    "\n",
    "$$\\omega_{AB} = \\dfrac{1}{\\Phi_A*n_{AB}}$$\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the importance of each journal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "journal_weights = dict()\n",
    "\n",
    "for paper in pr_list:\n",
    "    publication_id = paper[0]\n",
    "    node_centrality = paper[1]\n",
    "    if publication_id in reverse_nodes:\n",
    "        if reverse_nodes[publication_id][2] not in journal_weights:\n",
    "            journal_weights[reverse_nodes[publication_id][2]] = [0,0]\n",
    "        journal_weights[reverse_nodes[publication_id][2]][0] += node_centrality\n",
    "        journal_weights[reverse_nodes[publication_id][2]][1] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the importance value of journals into <i>journal_weights</i>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for journal in journal_weights:\n",
    "    journal_weights[journal] = journal_weights[journal][0] #/ journal_weights[journal][1]\n",
    "#journal_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve citations between journals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 189697/189697 [00:00<00:00, 557547.00it/s]\n"
     ]
    }
   ],
   "source": [
    "journal_citations = dict()\n",
    "article_citations = dict()\n",
    "\n",
    "# Iterate over citations_dict to build a journals citations' network\n",
    "for citation_obj in tqdm(citations_dict):\n",
    "    source = citation_obj['source']\n",
    "    target = citation_obj['target']\n",
    "    if source in nodes:\n",
    "        if target in nodes:\n",
    "            source_article = nodes[source][0]\n",
    "            target_article = nodes[target][0]\n",
    "            if source_article != target_article:\n",
    "                if source_article not in article_citations:\n",
    "                    article_citations[source_article] = list()\n",
    "                article_citations[source_article].append(target_article)\n",
    "                source_journal = nodes[source][1]\n",
    "                target_journal = nodes[target][1]\n",
    "                if source_journal in journals_dict:\n",
    "                    if target_journal in journals_dict:\n",
    "                        jorunal_source_id = journals_dict[source_journal]\n",
    "                        journal_target_id = journals_dict[target_journal]\n",
    "                        if jorunal_source_id not in journal_citations:\n",
    "                            journal_citations[jorunal_source_id] = list()\n",
    "                        journal_citations[jorunal_source_id].append(journal_target_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the second network:\n",
    "- journals_network -> such network will have the different journals as nodes; the edges will be weighted with the reciprocal of the number of citations of articles that goes from journal A to journal B. To be more accurate, it is correct to specify that target nodes without citations won't be considered at all, giving thus the possibility to avoid the definition of a normalization constant (that could have been useful to avoid 0-weigths in paths).\n",
    "\n",
    "\n",
    "</br>\n",
    "Also in this case, we will build the undirected version of this network, useful then to analyze its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the citations graph\n",
    "undirected_journals_network = nx.Graph()\n",
    "journals_network = nx.DiGraph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Populate the networks by adding nodes and edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = dict()\n",
    "\n",
    "for source_id in journal_citations:\n",
    "    memo = dict()\n",
    "    for target_id in journal_citations[source_id]:\n",
    "        if target_id not in memo:\n",
    "            memo[target_id] = 0\n",
    "        memo[target_id] += 1\n",
    "    for cited_journal in memo:\n",
    "        weights[(source_id, cited_journal)] = 1/(journal_weights[source_id]*memo[cited_journal])\n",
    "        undirected_journals_network.add_edge(source_id, cited_journal)\n",
    "        journals_network.add_edge(source_id, cited_journal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the undirected version of journals' network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_gml(undirected_journals_network, \"../gml format networks/undirected_journals_network.gml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign edge_attributes to the network, according to the previously computed weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.set_edge_attributes(journals_network, weights, \"relative_weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the <i>Betweenness Centrality</i> measure to retrieve the most important journals. The parameter \"weight\" will contain the weights attributed to the network in the previous snippet.</br>\n",
    "The \"normalized=True\" attribute is useful, in this case, because provides a normalization measure for the direct network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "journals_weighted_betweennes = nx.betweenness_centrality(journals_network, k=None, normalized=True, weight='relative_weights', endpoints=False, seed=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the 100 most influential journals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(24, 0.2963248760378571),\n",
       " (30, 0.07509842353747775),\n",
       " (112, 0.05213628249069465),\n",
       " (156, 0.04795837894170267),\n",
       " (95, 0.027001455513185477),\n",
       " (53, 0.02333163908518093),\n",
       " (44, 0.021540190209535206),\n",
       " (66, 0.019991451747958317),\n",
       " (9, 0.019918769751051675),\n",
       " (41, 0.01647652165442708)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "journals_influence = sorted(journals_weighted_betweennes.items(), key=lambda item: item[1], reverse=True)\n",
    "journals_influence[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the title of the most influential journal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Journal Of Virology'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for journal_title in journals_dict:\n",
    "    if journals_dict[journal_title] == journals_influence[0][0]:\n",
    "        most_influential_journal = journal_title\n",
    "        break\n",
    "most_influential_journal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of outgoing edges from each article in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw count of how many articles each specific article cites\n",
    "article_citations_tot = dict()\n",
    "\n",
    "for citation in citations_dict:\n",
    "    if citation['source'] in nodes:\n",
    "        source_article_id = nodes[citation['source']][0]\n",
    "        if citation['target'] in nodes:\n",
    "            target_article_id = nodes[citation['target']][0]\n",
    "            if source_article_id != target_article_id:\n",
    "                if source_article_id not in article_citations_tot:\n",
    "                    article_citations_tot[source_article_id] = 0\n",
    "                article_citations_tot[source_article_id] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a \"journals_sizes\" dictionary, containing pairs \"journal_id: journal_size\", retrieved by the betweenness centrality dictionary computed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "journal_influences = journals_weighted_betweennes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">In the following snippet, is given a weight to citations between articles.</br>\n",
    "Such weight is computed in the following way:\n",
    "- $n$ is the raw count of out-going citations from a certain article;\n",
    "- $\\alpha$ is the influence of the specific journal containing the citing article (computed with the betweenness centrality measure);\n",
    "- $\\lambda$ is a constant ($\\lambda = 0.1$) that is useful to normalize weights equal to $0$;\n",
    "</br>\n",
    "Following a flow of information that goes from the source article to the cited one, the relative weight ($\\Phi_{ij}$) of the connection between \"article $A$\" and \"article $B$\" is computed as follows:</br>\n",
    "\n",
    "$$\\Phi_{AB} = \\dfrac{\\alpha + \\lambda}{n}$$ \n",
    "</br>\n",
    "\n",
    "The idea behind this computation derives from the will to distribute the importance of a certain article between all the articles that it cites in an equal way. Furthermore, higher the number of cited articles -> smaller the importance passed to each one of them.</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_weights = dict()\n",
    "\n",
    "for paper in pr_list:\n",
    "    paper_id = paper[0]\n",
    "    paper_weight = paper[1]\n",
    "    paper_weights[paper_id] = paper_weight\n",
    "#paper_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a new network, that is the citation network of publications contained within the most influential journal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "publications_network = nx.DiGraph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add edges to the network and save the weights of these connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 189697/189697 [00:07<00:00, 24839.74it/s]\n"
     ]
    }
   ],
   "source": [
    "# articles_weights contains pairs of \"(tuple source-target): weight of the connection\"\n",
    "articles_weights = dict()\n",
    "\n",
    "for citation in tqdm(citations_dict):\n",
    "    found_all = False\n",
    "    if citation['source'] in nodes:\n",
    "        source_article_id = nodes[citation['source']][0]\n",
    "        source_journal = nodes[citation['source']][1]\n",
    "        if source_journal in journals_dict:\n",
    "            source_journal_id = journals_dict[source_journal]\n",
    "            if source_journal_id in journal_influences:\n",
    "                if source_article_id in article_citations_tot:\n",
    "                    article_distributed_weight = ((journal_influences[source_journal_id]/article_citations_tot[source_article_id])*paper_weights[source_article_id])\n",
    "                    found_all = True\n",
    "    if found_all:\n",
    "        if source_article_id in article_citations:\n",
    "            for cited_article_id in article_citations[source_article_id]:\n",
    "                if source_article_id != cited_article_id:\n",
    "                    publications_network.add_edge(source_article_id, cited_article_id)\n",
    "                    articles_weights[(source_article_id, cited_article_id)] = article_distributed_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the weights of edges within the most influential journal citations network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.set_edge_attributes(publications_network, articles_weights, \"relative_new_nodes_weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, compute the <i>Eigenvector Centrality</i> measure in order to find which publications can be identified as key publications within the reference context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_papers = nx.eigenvector_centrality(publications_network, max_iter=1000, weight='relative_new_nodes_weights')\n",
    "key_papers_list = sorted(key_papers.items(), key=lambda item: item[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(25837, 0.004485515506349722),\n",
       " (13497, 0.004485417976788674),\n",
       " (17440, 0.00448537379024872),\n",
       " (39264, 0.004485353130516605),\n",
       " (25492, 0.004485224651984496),\n",
       " (34785, 0.004485221858837908),\n",
       " (2883, 0.0044852197613752835),\n",
       " (21306, 0.004485206673108011),\n",
       " (26129, 0.004485197539569766),\n",
       " (26347, 0.004485112422437013)]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "three_key_papers = sorted(key_papers.items(), key=lambda item: item[1], reverse=True)[:10]\n",
    "three_key_papers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare these 3 articles with the 3 articles found at the beginning (that is, before assignign weights on the basis of the provenance's journals), in order to see whether our process led to different results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Old key papers       ---------     New key papers\n",
      "(39264, 0.003958045367325669) -- (25837, 0.004485515506349722)\n",
      "(17440, 0.0034456482660907505) -- (13497, 0.004485417976788674)\n",
      "(21306, 0.0031866534313306017) -- (17440, 0.00448537379024872)\n",
      "(25837, 0.002695082402101789) -- (39264, 0.004485353130516605)\n",
      "(12204, 0.002360879446452903) -- (25492, 0.004485224651984496)\n",
      "(26129, 0.002305909452775167) -- (34785, 0.004485221858837908)\n",
      "(12343, 0.002106388438053257) -- (2883, 0.0044852197613752835)\n",
      "(7812, 0.0020691617187695984) -- (21306, 0.004485206673108011)\n",
      "(40899, 0.002012513218906165) -- (26129, 0.004485197539569766)\n",
      "(8523, 0.001937907687581662) -- (26347, 0.004485112422437013)\n"
     ]
    }
   ],
   "source": [
    "# old key papers\n",
    "i=0\n",
    "print(\"    Old key papers\", \"      ---------     \" \"New key papers\")\n",
    "for el in pr_list[:10]:\n",
    "    print(el, \"--\", three_key_papers[i])\n",
    "    i+=1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the three articles we have retrieved now, with the three extracted at the beginning by means of the Eigenvector centrality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Old key papers       ---------     New key papers\n",
      "(34948, 0.22629370522315476) -- (25837, 0.004485515506349722)\n",
      "(26230, 0.19000624260331436) -- (13497, 0.004485417976788674)\n",
      "(3907, 0.18100263575061926) -- (17440, 0.00448537379024872)\n",
      "(11156, 0.17289612871374369) -- (39264, 0.004485353130516605)\n",
      "(23638, 0.16595863496464636) -- (25492, 0.004485224651984496)\n",
      "(17015, 0.1644623028002411) -- (34785, 0.004485221858837908)\n",
      "(4553, 0.1492088280751363) -- (2883, 0.0044852197613752835)\n",
      "(38019, 0.14674225337814517) -- (21306, 0.004485206673108011)\n",
      "(22743, 0.1426157058405683) -- (26129, 0.004485197539569766)\n",
      "(37554, 0.1410142599703751) -- (26347, 0.004485112422437013)\n"
     ]
    }
   ],
   "source": [
    "# old key papers\n",
    "i=0\n",
    "print(\"    Old key papers\", \"      ---------     \" \"New key papers\")\n",
    "for el in ec_list[:10]:\n",
    "    print(el, \"--\", three_key_papers[i])\n",
    "    i+=1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same is done for the first three articles extracted with the in degree count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Old key papers       ---------     New key papers\n",
      "(39264, 1026) -- (25837, 0.004485515506349722)\n",
      "(25837, 1014) -- (13497, 0.004485417976788674)\n",
      "(17440, 985) -- (17440, 0.00448537379024872)\n",
      "(21306, 794) -- (39264, 0.004485353130516605)\n",
      "(12204, 726) -- (25492, 0.004485224651984496)\n",
      "(8523, 623) -- (34785, 0.004485221858837908)\n",
      "(7573, 623) -- (2883, 0.0044852197613752835)\n",
      "(7812, 565) -- (21306, 0.004485206673108011)\n",
      "(26129, 484) -- (26129, 0.004485197539569766)\n",
      "(4212, 451) -- (26347, 0.004485112422437013)\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "print(\"    Old key papers\", \"      ---------     \" \"New key papers\")\n",
    "for el in in_d_list[:10]:\n",
    "    print(el, \"--\", three_key_papers[i])\n",
    "    i+=1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, there are differences with respect to the eigenvector, the pagerank and the in-degree."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we study how the final output of our workflow correlates with the yet known measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17440, 0.6276473389613471)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_key_papers = scaling(key_papers_list, -1, 1)\n",
    "scaled_key_papers = sort_list_of_tuples(scaled_key_papers)\n",
    "key_papers_array = [el[1] for el in scaled_key_papers]\n",
    "scaled_key_papers[17440]\n",
    "#key_papers_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation coefficient between the Eigenvector measure and the citation count: 0.49966997156315573\n",
      "Correlation coefficient between the PageRank measure and the citation count: 0.7095266980700551\n",
      "Correlation coefficient between our algorithm and the citation count: 0.7386009881982776\n"
     ]
    }
   ],
   "source": [
    "r, p = scipy.stats.kendalltau(key_papers_array, eigenvector_array)\n",
    "print(\"Correlation coefficient between the Eigenvector measure and the citation count:\", r)\n",
    "r, p = scipy.stats.kendalltau(key_papers_array, pagerank_array)\n",
    "print(\"Correlation coefficient between the PageRank measure and the citation count:\", r)\n",
    "r, p = scipy.stats.kendalltau(key_papers_array, in_degree_array)\n",
    "print(\"Correlation coefficient between our algorithm and the citation count:\", r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally retrieve metadata about these new key papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for paper in metadata_dict:\n",
    "    if paper['node_id'] == three_key_papers[0][0]:\n",
    "        key_paper_1 = paper\n",
    "    if paper['node_id'] == three_key_papers[1][0]:\n",
    "        key_paper_2 = paper\n",
    "    if paper['node_id'] == three_key_papers[2][0]:\n",
    "        key_paper_3 = paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '10.1056/nejmoa1211721',\n",
       " 'author': 'Zaki, Van Boheemen, Bestebroer, Osterhaus, Fouchier',\n",
       " 'year': '2012',\n",
       " 'title': 'Isolation Of A Novel Coronavirus From A Man With Pneumonia In Saudi Arabia',\n",
       " 'source_title': 'New England Journal Of Medicine',\n",
       " 'node_id': 25837}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_paper_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '10.1371/journal.ppat.1001258',\n",
       " 'author': 'Huang, Bailey, Weyer, Radoshitzky, Becker, Chiang, Brass, Ahmed, Chi, Dong, Longobardi, Boltz, Kuhn, Elledge, Bavari, Denison, Choe, Farzan',\n",
       " 'year': '2011',\n",
       " 'title': 'Distinct Patterns Of Ifitm-Mediated Restriction Of Filoviruses, Sars Coronavirus, And Influenza A Virus',\n",
       " 'source_title': 'Plos Pathogens',\n",
       " 'node_id': 13497}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_paper_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '10.1056/nejmoa030747',\n",
       " 'author': 'Drosten, Günther, Preiser, Van Der Werf, Brodt, Becker, Rabenau, Panning, Kolesnikova, Fouchier, Berger, Burguière, Cinatl, Eickmann, Escriou, Grywna, Kramme, Manuguerra, Müller, Rickerts, Stürmer, Vieth, Klenk, Osterhaus, Schmitz, Doerr',\n",
       " 'year': '2003',\n",
       " 'title': 'Identification Of A Novel Coronavirus In Patients With Severe Acute Respiratory Syndrome',\n",
       " 'source_title': 'New England Journal Of Medicine',\n",
       " 'node_id': 17440}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_paper_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the networks build during the entire process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_gml(papers_network, \"../gml format networks/directed_first_papers_network.gml\")\n",
    "nx.write_gml(journals_network, \"../gml format networks/directed_journals_network.gml\")\n",
    "nx.write_gml(publications_network, \"../gml format networks/directed_final_papers_network.gml\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
