{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Small world-ness, scale freedom, and modularity analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "import statistics as st\n",
    "import pandas as pd\n",
    "from math import log # is yet the natural logarithm ln"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build an undirected network for papers and one for journals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_network = nx.read_gml(\"../gml format networks/undirected_papers_network.gml\")\n",
    "journals_network = nx.read_gml(\"../gml format networks/undirected_journals_network.gml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small worldness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two values $\\sigma$ and $\\omega$ allow to determine whether a network is charcaterized by a small-world effect or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The small world coefficient $\\sigma$ is computed as:\n",
    "</br>\n",
    "$$\\sigma= \\dfrac{\\dfrac{C}{C_r}}{\\dfrac{L}{L_r}}$$\n",
    "</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $C$ is the clustering coefficient of our network and $C_r$ is the clusetering coefficient of an equivalent random graph, while $L$ and $L_r$ are the average shortest path values for our network and an equivalent random graph, respectively.\n",
    "\n",
    "A graph is commonly classified as small-world if $\\sigma \\gt 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The small world coefficient $\\omega$ is computed as:\n",
    "</br>\n",
    "$$\\omega=\\dfrac{L_r}{L}-\\dfrac{C}{C_l}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $C$ and $L$ are respectively the average clustering coefficient and average shortest path length of our network, while $L_r$ is the average shortest path length of an equivalent random graph and $C_l$ is the average clustering coefficient of an equivalent lattice graph.\n",
    "\n",
    "$\\omega$ measures if our graph is like a lattice or a random graph. Negative values mean that our network is similar to a lattice, whereas positive values mean that it is more likely to be similar to a random graph. Values close to 0 mean that it has small-world characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Network of papers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.$\\sigma$) Computing the sigma value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Computing the clustering coefficient $C$ and the average path length $L$ of our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute C\n",
    "C = nx.average_clustering(papers_network)\n",
    "print(\"C = \", C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute L\n",
    "L_list = list() # list of L values\n",
    "for N in (papers_network.subgraph(n).copy() for n in nx.connected_components(papers_network)): # for each connected component of the network (N) \n",
    "    L_list.append(nx.average_shortest_path_length(N)) # compute L and append it to the list\n",
    "\n",
    "if len(L_list) > 1:\n",
    "    computable_L_list = [L for L in L_list if L != 1.0] # remove the 0 values\n",
    "    if len(computable_L_list) == 1:\n",
    "        L = computable_L_list[0]\n",
    "    else:\n",
    "        L = st.mean(computable_L_list)\n",
    "else:\n",
    "    L = L_list[0]\n",
    "\n",
    "print(\"L = \", L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Retrieve the number of edges $E$ and nodes $n$ in the network, and the probability $p$ of finding an edge between two random nodes in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = papers_network.nx.number_of_edges()\n",
    "n = papers_network.nx.number_of_nodes()\n",
    "p = (2*E)/((n-1)*n)\n",
    "\n",
    "print(\"E = \", E)\n",
    "print(\"n = \", n)\n",
    "print(\"p = \", p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a random graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### I) ER with nodes and probability <span style=\"color:red\">!!!!! NON È UN ER RANDOM GRAPH !!!!</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we compute ten <i>ER random graphs</i> with respect to the probability value just obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "er_clusters_list = list()\n",
    "er_shortest_list = list()\n",
    "for network in tqdm(range(0, 11)):\n",
    "    G = nx.erdos_renyi_graph(n,p)\n",
    "    er_clusters_list.append(nx.average_clustering(G))\n",
    "    er_shortest_list.append(nx.average_shortest_path_length(G))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the average clustering coefficient and the average shortest path of the ER random graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_r = st.mean(er_shortest_list)\n",
    "C_r = st.mean(er_clusters_list)\n",
    "print(\"Average clustering coefficient: \", st.mean(er_clusters_list), \"standard deviation: \", st.stdev(er_clusters_list))\n",
    "print(\"Average shortest path: \", st.mean(er_shortest_list), \"standard deviation: \", st.stdev(er_shortest_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the Sigma value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = C/C_r\n",
    "D = L/L_r\n",
    "sigma_value = N/D\n",
    "print(N, D, sigma_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### II) ER with nodes and edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we compute ten <i>ER random graphs</i> with respect to the number of nodes and edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [05:36<00:00, 30.57s/it]\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "er_clusters_list = list()\n",
    "er_shortest_list = list()\n",
    "for iteration in tqdm(range(0, 11)):\n",
    "    G = nx.gnm_random_graph(n,E)\n",
    "    er_clusters_list.append(nx.average_clustering(G))\n",
    "    er_shortest_list.append(nx.average_shortest_path_length(G))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the average clustering coefficient and the average shortest path of the ER random graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average clustering coefficient:  0.002382841486418715 standard deviation:  0.0001124815647467052\n",
      "Average shortest path:  3.5873621528786774 standard deviation:  0.0005222689601180479\n"
     ]
    }
   ],
   "source": [
    "L_r = st.mean(er_shortest_list)\n",
    "C_r = st.mean(er_clusters_list)\n",
    "print(\"Average clustering coefficient: \", st.mean(er_clusters_list), \"standard deviation: \", st.stdev(er_clusters_list))\n",
    "print(\"Average shortest path: \", st.mean(er_shortest_list), \"standard deviation: \", st.stdev(er_shortest_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the Sigma value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = C/C_r\n",
    "D = L/L_r\n",
    "sigma_value = N/D\n",
    "print(N, D, sigma_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### L'AFFERMAZIONE SOTTO È DA VEDERE, ANCHE SE È QUELLO CHE CI ASPETTIAMO\n",
    "<span style=\"color:red\">In the end, the sigma value remains pretty similar for both the ways of computing ER random graphs.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Network of journals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.989171113335879]\n",
      "L =  2.989171113335879\n",
      "L =  2.989171113335879\n"
     ]
    }
   ],
   "source": [
    "L_list = list() # list of L values\n",
    "for N in (journals_network.subgraph(n).copy() for n in nx.connected_components(journals_network)): # for each connected component of the network (N) \n",
    "    L_list.append(nx.average_shortest_path_length(N)) # compute L and append it to the list\n",
    "\n",
    "if len(L_list) > 1:\n",
    "    computable_L_list = [L for L in L_list if L != 1.0] # remove the 0 values\n",
    "    if len(computable_L_list) == 1:\n",
    "        L = computable_L_list[0]\n",
    "    else:\n",
    "        L = st.mean(computable_L_list)\n",
    "else:\n",
    "    L = L_list[0]\n",
    "\n",
    "print(\"L = \", L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C =  0.33300640976926094\n"
     ]
    }
   ],
   "source": [
    "# Compute C\n",
    "C = nx.average_clustering(journals_network)\n",
    "print(\"C = \", C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve the number of edges $E$ and of nodes $n$ in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E =  39100\n",
      "n =  5661\n"
     ]
    }
   ],
   "source": [
    "E = journals_network.number_of_edges()\n",
    "n = journals_network.number_of_nodes()\n",
    "print(\"E = \", E)\n",
    "print(\"n = \", n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the probability $p$ of finding an edge between two random nodes in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p =  0.002440603147316928\n"
     ]
    }
   ],
   "source": [
    "p = (E)/(((n-1)*n)/2)\n",
    "print(\"p = \", p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a random graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### I) ER with nodes and probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we compute ten <i>ER random graphs</i> with respect to the probability value just obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [16:19<00:00, 89.04s/it]\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "er_clusters_list = list()\n",
    "er_shortest_list = list()\n",
    "for iteration in tqdm(range(0, 11)):\n",
    "    G = nx.erdos_renyi_graph(n,p)\n",
    "    er_clusters_list.append(nx.average_clustering(G))\n",
    "    er_shortest_list.append(nx.average_shortest_path_length(G))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the average clustering coefficient and the average shortest path of the ER random graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average clustering coefficient:  0.002521754530160568 standard deviation:  0.00012973608362599\n",
      "Average shortest path:  3.589043371521027 standard deviation:  0.003637616392217271\n"
     ]
    }
   ],
   "source": [
    "L_r = st.mean(er_shortest_list)\n",
    "C_r = st.mean(er_clusters_list)\n",
    "print(\"Average clustering coefficient: \", st.mean(er_clusters_list), \"standard deviation: \", st.stdev(er_clusters_list))\n",
    "print(\"Average shortest path: \", st.mean(er_shortest_list), \"standard deviation: \", st.stdev(er_shortest_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132.05345952052573 0.8328601256409661 158.5541862973664\n"
     ]
    }
   ],
   "source": [
    "N = C/C_r\n",
    "D = L/L_r\n",
    "sigma_value = N/D\n",
    "print(N, D, sigma_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### II) ER with nodes and edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we compute ten <i>ER random graphs</i> with respect to the number of nodes and edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "er_clusters_list = list()\n",
    "er_shortest_list = list()\n",
    "for iteration in tqdm(range(0, 11)):\n",
    "    G = nx.gnm_random_graph(n,E)\n",
    "    er_clusters_list.append(nx.average_clustering(G))\n",
    "    er_shortest_list.append(nx.average_shortest_path_length(G))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the average clustering coefficient and the average shortest path of the ER random graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average clustering coefficient:  0.002382841486418715 standard deviation:  0.0001124815647467052\n",
      "Average shortest path:  3.5873621528786774 standard deviation:  0.0005222689601180479\n"
     ]
    }
   ],
   "source": [
    "L_r = st.mean(er_shortest_list)\n",
    "C_r = st.mean(er_clusters_list)\n",
    "print(\"Average clustering coefficient: \", st.mean(er_clusters_list), \"standard deviation: \", st.stdev(er_clusters_list))\n",
    "print(\"Average shortest path: \", st.mean(er_shortest_list), \"standard deviation: \", st.stdev(er_shortest_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139.7518096219451 0.8332504458567752 167.71885369739917\n"
     ]
    }
   ],
   "source": [
    "N = C/C_r\n",
    "D = L/L_r\n",
    "sigma_value = N/D\n",
    "print(N, D, sigma_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">In the end, the sigma value remains pretty similar for both the ways of computing ER random graphs.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute omega values.\n",
    "</br>\n",
    "$$\\omega=\\dfrac{L_r}{L}-\\dfrac{C}{C_l}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $L_r$ is computed from a random graph and, $C_l$ is computed from a lattice graph.\n",
    "</br>\n",
    "$\\omega$ should be between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seen that $L_r$, $L$ and $C$ have yet been computed, we just need to define $C_L$ for lattice graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latticized_jn = nx.lattice_reference(journals_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract $C_l$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_l = nx.average_clustering(latticized_jn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Scale free network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find degree of each node in the undirected graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 2434,\n",
      " 2: 815,\n",
      " 3: 428,\n",
      " 4: 252,\n",
      " 5: 155,\n",
      " 6: 144,\n",
      " 7: 108,\n",
      " 8: 88,\n",
      " 9: 85,\n",
      " 10: 57,\n",
      " 11: 50,\n",
      " 12: 61,\n",
      " 13: 34,\n",
      " 14: 54,\n",
      " 15: 29,\n",
      " 16: 30,\n",
      " 17: 43,\n",
      " 18: 25,\n",
      " 19: 26,\n",
      " 20: 24,\n",
      " 21: 31,\n",
      " 22: 26,\n",
      " 23: 22,\n",
      " 24: 13,\n",
      " 25: 23,\n",
      " 26: 15,\n",
      " 27: 14,\n",
      " 28: 20,\n",
      " 29: 18,\n",
      " 30: 9,\n",
      " 31: 12,\n",
      " 32: 15,\n",
      " 33: 15,\n",
      " 34: 8,\n",
      " 35: 11,\n",
      " 36: 13,\n",
      " 37: 12,\n",
      " 38: 19,\n",
      " 39: 8,\n",
      " 40: 13,\n",
      " 41: 12,\n",
      " 42: 9,\n",
      " 43: 9,\n",
      " 44: 10,\n",
      " 45: 7,\n",
      " 46: 14,\n",
      " 47: 4,\n",
      " 48: 4,\n",
      " 49: 10,\n",
      " 50: 4,\n",
      " 51: 8,\n",
      " 52: 4,\n",
      " 53: 6,\n",
      " 54: 1,\n",
      " 55: 9,\n",
      " 56: 3,\n",
      " 57: 3,\n",
      " 58: 3,\n",
      " 59: 6,\n",
      " 60: 5,\n",
      " 61: 8,\n",
      " 62: 8,\n",
      " 63: 8,\n",
      " 64: 1,\n",
      " 65: 2,\n",
      " 66: 3,\n",
      " 67: 4,\n",
      " 68: 5,\n",
      " 69: 4,\n",
      " 70: 4,\n",
      " 71: 2,\n",
      " 72: 2,\n",
      " 73: 4,\n",
      " 74: 2,\n",
      " 75: 5,\n",
      " 76: 4,\n",
      " 77: 3,\n",
      " 79: 2,\n",
      " 80: 5,\n",
      " 81: 3,\n",
      " 82: 1,\n",
      " 83: 1,\n",
      " 84: 2,\n",
      " 86: 1,\n",
      " 87: 2,\n",
      " 88: 1,\n",
      " 89: 7,\n",
      " 90: 2,\n",
      " 91: 3,\n",
      " 92: 6,\n",
      " 93: 4,\n",
      " 94: 3,\n",
      " 95: 7,\n",
      " 96: 3,\n",
      " 98: 2,\n",
      " 99: 1,\n",
      " 100: 3,\n",
      " 101: 2,\n",
      " 102: 1,\n",
      " 104: 2,\n",
      " 105: 2,\n",
      " 106: 1,\n",
      " 107: 3,\n",
      " 108: 5,\n",
      " 109: 2,\n",
      " 110: 3,\n",
      " 114: 1,\n",
      " 115: 2,\n",
      " 116: 1,\n",
      " 117: 1,\n",
      " 118: 1,\n",
      " 119: 1,\n",
      " 120: 1,\n",
      " 121: 3,\n",
      " 123: 2,\n",
      " 124: 1,\n",
      " 125: 1,\n",
      " 126: 4,\n",
      " 127: 1,\n",
      " 131: 1,\n",
      " 132: 2,\n",
      " 133: 3,\n",
      " 135: 1,\n",
      " 136: 2,\n",
      " 137: 2,\n",
      " 138: 1,\n",
      " 143: 2,\n",
      " 144: 2,\n",
      " 145: 1,\n",
      " 147: 2,\n",
      " 148: 1,\n",
      " 149: 1,\n",
      " 150: 2,\n",
      " 151: 1,\n",
      " 152: 1,\n",
      " 153: 1,\n",
      " 154: 1,\n",
      " 161: 1,\n",
      " 162: 2,\n",
      " 166: 1,\n",
      " 168: 1,\n",
      " 171: 1,\n",
      " 172: 3,\n",
      " 177: 1,\n",
      " 183: 1,\n",
      " 185: 1,\n",
      " 186: 1,\n",
      " 187: 2,\n",
      " 188: 1,\n",
      " 189: 1,\n",
      " 190: 1,\n",
      " 191: 1,\n",
      " 192: 1,\n",
      " 203: 1,\n",
      " 207: 1,\n",
      " 211: 1,\n",
      " 217: 1,\n",
      " 219: 1,\n",
      " 227: 2,\n",
      " 229: 1,\n",
      " 232: 1,\n",
      " 233: 1,\n",
      " 245: 1,\n",
      " 246: 2,\n",
      " 249: 1,\n",
      " 252: 1,\n",
      " 253: 2,\n",
      " 266: 1,\n",
      " 272: 1,\n",
      " 274: 1,\n",
      " 275: 1,\n",
      " 276: 1,\n",
      " 277: 1,\n",
      " 286: 1,\n",
      " 294: 1,\n",
      " 295: 1,\n",
      " 307: 1,\n",
      " 321: 1,\n",
      " 322: 1,\n",
      " 326: 1,\n",
      " 342: 1,\n",
      " 405: 1,\n",
      " 432: 1,\n",
      " 452: 1,\n",
      " 459: 1,\n",
      " 485: 1,\n",
      " 491: 1,\n",
      " 517: 1,\n",
      " 533: 1,\n",
      " 553: 1,\n",
      " 569: 1,\n",
      " 570: 1,\n",
      " 617: 1,\n",
      " 660: 1,\n",
      " 694: 1,\n",
      " 706: 1,\n",
      " 715: 1,\n",
      " 721: 1,\n",
      " 724: 1,\n",
      " 729: 1,\n",
      " 785: 1,\n",
      " 893: 1,\n",
      " 898: 1,\n",
      " 986: 1,\n",
      " 1556: 1}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "degree_dict = dict()\n",
    "for node in journals_network:\n",
    "    degree = journals_network.degree(node)\n",
    "    if degree not in degree_dict:\n",
    "        degree_dict[degree] = 0\n",
    "    degree_dict[degree] += 1\n",
    "\n",
    "pprint(degree_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Index(...) must be called with a collection of some kind, 205 was passed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/lorenzo/Desktop/Citations' Network Analysis/COVID-19-Citations-Network-Analysis/Notebooks/networks_structure.ipynb Cella 54\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lorenzo/Desktop/Citations%27%20Network%20Analysis/COVID-19-Citations-Network-Analysis/Notebooks/networks_structure.ipynb#Y116sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lorenzo/Desktop/Citations%27%20Network%20Analysis/COVID-19-Citations-Network-Analysis/Notebooks/networks_structure.ipynb#Y116sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/lorenzo/Desktop/Citations%27%20Network%20Analysis/COVID-19-Citations-Network-Analysis/Notebooks/networks_structure.ipynb#Y116sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mDataFrame(degree_dict, \u001b[39mlen\u001b[39;49m(degree_dict))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lorenzo/Desktop/Citations%27%20Network%20Analysis/COVID-19-Citations-Network-Analysis/Notebooks/networks_structure.ipynb#Y116sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m df\u001b[39m.\u001b[39mtranspose()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/frame.py:636\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    630\u001b[0m     mgr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_mgr(\n\u001b[1;32m    631\u001b[0m         data, axes\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m\"\u001b[39m: index, \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: columns}, dtype\u001b[39m=\u001b[39mdtype, copy\u001b[39m=\u001b[39mcopy\n\u001b[1;32m    632\u001b[0m     )\n\u001b[1;32m    634\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, \u001b[39mdict\u001b[39m):\n\u001b[1;32m    635\u001b[0m     \u001b[39m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 636\u001b[0m     mgr \u001b[39m=\u001b[39m dict_to_mgr(data, index, columns, dtype\u001b[39m=\u001b[39;49mdtype, copy\u001b[39m=\u001b[39;49mcopy, typ\u001b[39m=\u001b[39;49mmanager)\n\u001b[1;32m    637\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ma\u001b[39m.\u001b[39mMaskedArray):\n\u001b[1;32m    638\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mma\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmrecords\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mmrecords\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/internals/construction.py:502\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    494\u001b[0m     arrays \u001b[39m=\u001b[39m [\n\u001b[1;32m    495\u001b[0m         x\n\u001b[1;32m    496\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(x, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(x\u001b[39m.\u001b[39mdtype, ExtensionDtype)\n\u001b[1;32m    497\u001b[0m         \u001b[39melse\u001b[39;00m x\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m    498\u001b[0m         \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m arrays\n\u001b[1;32m    499\u001b[0m     ]\n\u001b[1;32m    500\u001b[0m     \u001b[39m# TODO: can we get rid of the dt64tz special case above?\u001b[39;00m\n\u001b[0;32m--> 502\u001b[0m \u001b[39mreturn\u001b[39;00m arrays_to_mgr(arrays, columns, index, dtype\u001b[39m=\u001b[39;49mdtype, typ\u001b[39m=\u001b[39;49mtyp, consolidate\u001b[39m=\u001b[39;49mcopy)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/internals/construction.py:122\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    120\u001b[0m     index \u001b[39m=\u001b[39m _extract_index(arrays)\n\u001b[1;32m    121\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 122\u001b[0m     index \u001b[39m=\u001b[39m ensure_index(index)\n\u001b[1;32m    124\u001b[0m \u001b[39m# don't force copy because getting jammed in an ndarray anyway\u001b[39;00m\n\u001b[1;32m    125\u001b[0m arrays \u001b[39m=\u001b[39m _homogenize(arrays, index, dtype)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/indexes/base.py:7043\u001b[0m, in \u001b[0;36mensure_index\u001b[0;34m(index_like, copy)\u001b[0m\n\u001b[1;32m   7041\u001b[0m         \u001b[39mreturn\u001b[39;00m Index\u001b[39m.\u001b[39m_with_infer(index_like, copy\u001b[39m=\u001b[39mcopy, tupleize_cols\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m   7042\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 7043\u001b[0m     \u001b[39mreturn\u001b[39;00m Index\u001b[39m.\u001b[39;49m_with_infer(index_like, copy\u001b[39m=\u001b[39;49mcopy)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/indexes/base.py:680\u001b[0m, in \u001b[0;36mIndex._with_infer\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[39mwith\u001b[39;00m warnings\u001b[39m.\u001b[39mcatch_warnings():\n\u001b[1;32m    679\u001b[0m     warnings\u001b[39m.\u001b[39mfilterwarnings(\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m.*the Index constructor\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFutureWarning\u001b[39;00m)\n\u001b[0;32m--> 680\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    682\u001b[0m \u001b[39mif\u001b[39;00m result\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m _dtype_obj \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m result\u001b[39m.\u001b[39m_is_multi:\n\u001b[1;32m    683\u001b[0m     \u001b[39m# error: Argument 1 to \"maybe_convert_objects\" has incompatible type\u001b[39;00m\n\u001b[1;32m    684\u001b[0m     \u001b[39m# \"Union[ExtensionArray, ndarray[Any, Any]]\"; expected\u001b[39;00m\n\u001b[1;32m    685\u001b[0m     \u001b[39m# \"ndarray[Any, Any]\"\u001b[39;00m\n\u001b[1;32m    686\u001b[0m     values \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39mmaybe_convert_objects(result\u001b[39m.\u001b[39m_values)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/indexes/base.py:508\u001b[0m, in \u001b[0;36mIndex.__new__\u001b[0;34m(cls, data, dtype, copy, name, tupleize_cols, **kwargs)\u001b[0m\n\u001b[1;32m    505\u001b[0m     \u001b[39mreturn\u001b[39;00m klass\u001b[39m.\u001b[39m_simple_new(arr, name)\n\u001b[1;32m    507\u001b[0m \u001b[39melif\u001b[39;00m is_scalar(data):\n\u001b[0;32m--> 508\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_scalar_data_error(data)\n\u001b[1;32m    509\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mhasattr\u001b[39m(data, \u001b[39m\"\u001b[39m\u001b[39m__array__\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    510\u001b[0m     \u001b[39mreturn\u001b[39;00m Index(np\u001b[39m.\u001b[39masarray(data), dtype\u001b[39m=\u001b[39mdtype, copy\u001b[39m=\u001b[39mcopy, name\u001b[39m=\u001b[39mname, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[0;31mTypeError\u001b[0m: Index(...) must be called with a collection of some kind, 205 was passed"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(degree_dict, len(degree_dict))\n",
    "df.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find $\\alpha$ (scaling coefficient: if $2<\\alpha<3$, then the network is a scale free newtork)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5434686393382082"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_for_alpha = 0\n",
    "for degree in degree_dict:\n",
    "    multiplier = degree_dict[degree]\n",
    "    to_add = log(degree/(1-1/2))\n",
    "    sum_for_alpha += to_add*multiplier\n",
    "        \n",
    "alpha = 1+n*((sum_for_alpha)**-1)\n",
    "alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Modularity coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute $Q$, that is the modularity coefficient."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
